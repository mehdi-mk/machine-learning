{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise: multi-class classification on Stack Overflow questions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d940fa2372f765a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf  \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T15:37:47.595391900Z",
     "start_time": "2023-09-21T15:37:31.522763300Z"
    }
   },
   "id": "8782f05eae5ff245"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
      "6053168/6053168 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('stack_overflow_16k', url, untar=True, cache_dir='.', cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'stack_overflow')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:52:09.310416100Z",
     "start_time": "2023-09-21T16:51:22.331827800Z"
    }
   },
   "id": "cd2f8937790c9abe"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['README.md', 'test', 'train']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:56:51.549976900Z",
     "start_time": "2023-09-21T16:56:51.491833900Z"
    }
   },
   "id": "165a26aad89b513d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'.\\\\stack_overflow\\\\train'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "train_dir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:57:43.230678400Z",
     "start_time": "2023-09-21T16:57:43.200914400Z"
    }
   },
   "id": "2c0f53c51aca154a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "['csharp', 'java', 'javascript', 'python']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:57:59.531918200Z",
     "start_time": "2023-09-21T16:57:59.501489600Z"
    }
   },
   "id": "e99f9b43afc8dc23"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"float rounding problems in blank amount = 0.002638309660058967.price = 1392.18..lowest_ask = 1391.6..result = price*amount/lowest_ask..print(result)...the above code will print out:..0.002639409271731024...however when i perform the calculation here: http://web2.0calc.com/.it gives me: 0.0026394092717310237698..so obviously blank is rounding up the result of this calculation......my question is, how do you prevent blank from rounding up result? i.e. i want result to be: 0.002639409271731023\"\n"
     ]
    }
   ],
   "source": [
    "sample_file = os.path.join(train_dir, 'python/15.txt')\n",
    "with open(sample_file) as f:\n",
    "    print(f.read())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T16:59:00.780263500Z",
     "start_time": "2023-09-21T16:59:00.710104900Z"
    }
   },
   "id": "1028447e939c8f81"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'stack_overflow/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:27:14.594111600Z",
     "start_time": "2023-09-21T17:27:13.818981500Z"
    }
   },
   "id": "f8bc072330306c7b"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions b'\"blank8 why is my solution faster than the neat solution? (hackerrank chocolate feast) edit: simplified my solution..edit: removed opinion based secondary question...background: atarted learning blank a week or two ago using hackerranks problems as exercises and stackoverflow search + google as my teacher, i\\'ve had some limited experience learning other languages...i did the exercise my own \"\"noobish learner way\"\" which i can\\'t help but feel is a \"\"botched job\"\" when i see \"\"neat &amp; short\"\" solutions...however, when submitting both solutions one after another a couple of times i found the \"\"neat\"\" solution was quite a bit slower. ..i vaguely remember something about % operations being costly, is mine faster because of no % operations or is there more to it than just that?..exercise: https://www.hackerrank.com/challenges/chocolate-feast..neat solution from discussion:..import blank.io.*;.import blank.util.*;..public class solution {.    static int cc; .    public static void main(string[] args) {.        scanner in = new scanner(system.in);.        int t,n,c,m,r;.            t = in.nextint();.            while(t--&gt;0){.             n = in.nextint();.            c = in.nextint();.             m = in.nextint();.                r=n/c;.                cc=r;..                    while(r&gt;=m){.                        cc=cc+r/m;.                        r=r%m+r/m;.                    }..                system.out.println(cc); .            }..    }.}...my solution:..import blank.io.*;.import blank.util.*;..public class solution {..    public static void main(string[] args) {..        scanner sc = new scanner(system.in);.        int t = integer.parseint(sc.nextline());    //t = number of test cases.        int[][] tc = readinput(sc, t);              //tc[t][0] = money. tc[t][1] = price. tc[t][2] = wrappers per free bar..        for (int i = 0; i&lt;t; i++){                  //loop for all test cases.            int choc = calcchoc(tc,i);              //work out how much choc can be bought.            system.out.println(choc);               //print result for the test case.        }.    }.    //calculate how much choc he can buy with m $ at p price with w wrappers needed for a free bar.    public static int calcchoc(int[][] tc,int i){..        int m = tc[i][0];       //money he has.        int p = tc[i][1];       //price of choc.        int w = tc[i][2];       //wrappers per free bar..        int bars = m/p;         //how many bars he can buy initially.        int wrappers = bars;    //each bar is a wrapper from initial purpose..        //loop to turn in all wrappers while it is possible to do so.        while (w&lt;=wrappers){..            int barsfromturnin = wrappers/w;                //bars from turning in current wrappers..            bars = bars + barsfromturnin;                   //new bar count.            wrappers = wrappers - (barsfromturnin * (w-1)); //wrapper count reduced by amount of wrappers turned in -1 wrapper per bar recieved from turn in...            if (w==1){ //break out of infinite loop when you get 1 bar for 1 wrapper!.                system.out.print(\"\"infinite bars, exiting infinite loop at bars = \"\");.                break;.            }.        }.        return bars;.    }.    //read input for each test case and make 2d array of the info.    public static int[][] readinput(scanner sc, int t){..        int[][] input = new int[t][3];..        for (int i = 0; i&lt;t; i++){.            string[] inputline = sc.nextline().split(\"\" \"\");..            input[i][0] = integer.parseint(inputline[0]);.            input[i][1] = integer.parseint(inputline[1]);.            input[i][2] = integer.parseint(inputline[2]);.        }.        return input;.    }.}\"\\n'\n",
      "Label 1\n",
      "Questions b'\"element.removeeventlistener(\\'mousedown\\', externalfunction, usecapture); is not working i need to first remove the event listener before dynamically adding more elements which also need the same event listener. i am using an external function name (not an anonymous function) and specifying the same usecapture value in both the add and remove. ..the function is nested within another function. &lt; suspected problem was the problem..you can see the problem by clicking the first \"\"add button\"\"  more than once. the first click adds one more button, the second click adds two more, the third click adds four more, etc. each click should only add one more. i guess the return value of removeeventlistener is always undefined so i can only tell that removal did not work from the duplicate events.....var app = function() {. console.log(\\'app\\');. . var setup = function() {.  console.log(\\'setup\\');. .  var addbutton = function(e) {.   console.log(e);.   var button = e.target;.   var newbutton = document.createelement(\\'button\\');.   newbutton.innertext = \\'add another button\\';.   button.parentnode.appendchild( newbutton );.   setup();.  }. .  var buttons = document.queryselectorall(\\'button\\');.  .  for(var i=0; i&lt;buttons.length; i++) {.   var button = buttons[i];.   button.removeeventlistener(\\'mousedown\\', addbutton, false);.   button.addeventlistener(\\'mousedown\\', addbutton, false);.  }.  . }. setup();.}.app();.&lt;div&gt;. &lt;button&gt;add button&lt;/button&gt;.&lt;/div&gt;\"\\n'\n",
      "Label 2\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(2):\n",
    "        print(\"Questions\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:27:21.979174100Z",
     "start_time": "2023-09-21T17:27:21.889178700Z"
    }
   },
   "id": "a051d8592115cd2c"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "'csharp'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_ds.class_names[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:28:03.419497400Z",
     "start_time": "2023-09-21T17:28:03.339393300Z"
    }
   },
   "id": "b347129f27a905b7"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'java'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_ds.class_names[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:28:17.639475500Z",
     "start_time": "2023-09-21T17:28:17.549191100Z"
    }
   },
   "id": "a7b94f46892f3e4f"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'javascript'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_ds.class_names[2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:28:21.739414700Z",
     "start_time": "2023-09-21T17:28:21.679414900Z"
    }
   },
   "id": "656b19e6166824dd"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "'python'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_ds.class_names[3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:28:27.159253300Z",
     "start_time": "2023-09-21T17:28:27.099281100Z"
    }
   },
   "id": "1b3f195755097e7f"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'stack_overflow/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:29:27.924441Z",
     "start_time": "2023-09-21T17:29:27.199372700Z"
    }
   },
   "id": "fbdb1d827c04e3dd"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'stack_overflow/test',\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T17:30:06.909395300Z",
     "start_time": "2023-09-21T17:30:06.129606900Z"
    }
   },
   "id": "e3d35fc154cc1599"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:36:02.687299800Z",
     "start_time": "2023-09-21T18:36:02.667188400Z"
    }
   },
   "id": "14e8d0efa52ba3bb"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:36:07.367052800Z",
     "start_time": "2023-09-21T18:36:03.207064Z"
    }
   },
   "id": "59615e5d533f3aca"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:36:07.387382200Z",
     "start_time": "2023-09-21T18:36:07.377040600Z"
    }
   },
   "id": "76a06e8f86407b03"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  tf.Tensor(b'\"how to change data format in write function in blank? how to change the data format in f.write function? ..loaded_data = 349.00  or 3.00..i want to change data format in write function like %6f in print function. ..ex)  349.00 -> 349.000000 ,   3.00 -> 3.000000..f = open(\"\"test.txt\"\", \\'w\\').f.write( str.(loaded_data).zfill(?) )  ...what is the code that performs above function?\"\\n', shape=(), dtype=string)\n",
      "Label:  python\n",
      "Vectorized question:  (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[  90,    4,  174,   99,  375,    7,  167,   53,    7,  278,   41,\n",
      "           4,  174,    2,   99,  375,    7,    1, 1433,    1,    3,    1,\n",
      "          46,    1,   45,    4,  174,   99,  375,    7,  167,   53,   51,\n",
      "           1,    7,   88,  702,    1,    1, 2335,    1,  222,    1, 2335,\n",
      "           1,    3,    1,    1,    1,  367, 4370,    8,    2,   37,   15,\n",
      "        3890,  282, 5033,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=3>)\n"
     ]
    }
   ],
   "source": [
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_question, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Question: \", first_question)\n",
    "print(\"Label: \", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized question: \", vectorize_text(first_question, first_label))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:36:07.587463600Z",
     "start_time": "2023-09-21T18:36:07.387382200Z"
    }
   },
   "id": "8f9478d16e4a0494"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 =>  static\n",
      "550 =>  const\n"
     ]
    }
   ],
   "source": [
    "print(\"55 => \", vectorize_layer.get_vocabulary()[55])\n",
    "print(\"550 => \", vectorize_layer.get_vocabulary()[550])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:36:08.707399500Z",
     "start_time": "2023-09-21T18:36:08.647294Z"
    }
   },
   "id": "ea5a6c39d4bf5b0e"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:36:10.834577600Z",
     "start_time": "2023-09-21T18:36:10.727253900Z"
    }
   },
   "id": "6e9d089ccd4fc9c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuring the dataset for performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ff5b958e40a4f1b"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:36:11.707224200Z",
     "start_time": "2023-09-21T18:36:11.692109Z"
    }
   },
   "id": "b8e3aeb432f09ee6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a084c2da100ec2d2"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d_8  (None, 16)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160084 (625.33 KB)\n",
      "Trainable params: 160084 (625.33 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_features + 1, embedding_dim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(4)\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:38:00.207562Z",
     "start_time": "2023-09-21T18:38:00.097114800Z"
    }
   },
   "id": "16165ccf66c9ecba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function and optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26a5b774f0bb37a8"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:38:00.827203100Z",
     "start_time": "2023-09-21T18:38:00.797398600Z"
    }
   },
   "id": "4a631c7f31b6c42c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a38a71ff767aa781"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 1.3788 - accuracy: 0.3439 - val_loss: 1.3677 - val_accuracy: 0.4269\n",
      "Epoch 2/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.3495 - accuracy: 0.4727 - val_loss: 1.3270 - val_accuracy: 0.5200\n",
      "Epoch 3/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.2966 - accuracy: 0.5270 - val_loss: 1.2657 - val_accuracy: 0.5700\n",
      "Epoch 4/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.2267 - accuracy: 0.5928 - val_loss: 1.1919 - val_accuracy: 0.6356\n",
      "Epoch 5/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.1479 - accuracy: 0.6381 - val_loss: 1.1151 - val_accuracy: 0.6725\n",
      "Epoch 6/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 1.0688 - accuracy: 0.6903 - val_loss: 1.0417 - val_accuracy: 0.7044\n",
      "Epoch 7/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9961 - accuracy: 0.7222 - val_loss: 0.9769 - val_accuracy: 0.7212\n",
      "Epoch 8/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9313 - accuracy: 0.7473 - val_loss: 0.9192 - val_accuracy: 0.7400\n",
      "Epoch 9/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8756 - accuracy: 0.7623 - val_loss: 0.8689 - val_accuracy: 0.7500\n",
      "Epoch 10/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.8228 - accuracy: 0.7769 - val_loss: 0.8258 - val_accuracy: 0.7569\n",
      "Epoch 11/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7768 - accuracy: 0.7889 - val_loss: 0.7875 - val_accuracy: 0.7669\n",
      "Epoch 12/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7373 - accuracy: 0.8033 - val_loss: 0.7550 - val_accuracy: 0.7769\n",
      "Epoch 13/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7023 - accuracy: 0.8092 - val_loss: 0.7260 - val_accuracy: 0.7837\n",
      "Epoch 14/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.8166 - val_loss: 0.7001 - val_accuracy: 0.7919\n",
      "Epoch 15/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.8300 - val_loss: 0.6771 - val_accuracy: 0.7981\n",
      "Epoch 16/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.8327 - val_loss: 0.6563 - val_accuracy: 0.8006\n",
      "Epoch 17/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5872 - accuracy: 0.8419 - val_loss: 0.6382 - val_accuracy: 0.8031\n",
      "Epoch 18/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5623 - accuracy: 0.8509 - val_loss: 0.6207 - val_accuracy: 0.8075\n",
      "Epoch 19/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5404 - accuracy: 0.8587 - val_loss: 0.6060 - val_accuracy: 0.8100\n",
      "Epoch 20/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.8639 - val_loss: 0.5919 - val_accuracy: 0.8144\n",
      "Epoch 21/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4996 - accuracy: 0.8705 - val_loss: 0.5788 - val_accuracy: 0.8181\n",
      "Epoch 22/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4810 - accuracy: 0.8814 - val_loss: 0.5669 - val_accuracy: 0.8194\n",
      "Epoch 23/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4670 - accuracy: 0.8778 - val_loss: 0.5554 - val_accuracy: 0.8238\n",
      "Epoch 24/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4491 - accuracy: 0.8855 - val_loss: 0.5461 - val_accuracy: 0.8238\n",
      "Epoch 25/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4357 - accuracy: 0.8877 - val_loss: 0.5374 - val_accuracy: 0.8275\n",
      "Epoch 26/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4215 - accuracy: 0.8931 - val_loss: 0.5292 - val_accuracy: 0.8288\n",
      "Epoch 27/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4066 - accuracy: 0.8981 - val_loss: 0.5215 - val_accuracy: 0.8319\n",
      "Epoch 28/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3919 - accuracy: 0.8995 - val_loss: 0.5145 - val_accuracy: 0.8338\n",
      "Epoch 29/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3796 - accuracy: 0.9044 - val_loss: 0.5078 - val_accuracy: 0.8344\n",
      "Epoch 30/40\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3663 - accuracy: 0.9069 - val_loss: 0.5025 - val_accuracy: 0.8325\n",
      "Epoch 31/40\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.3548 - accuracy: 0.9112 - val_loss: 0.4975 - val_accuracy: 0.8356\n",
      "Epoch 32/40\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3446 - accuracy: 0.9134 - val_loss: 0.4916 - val_accuracy: 0.8325\n",
      "Epoch 33/40\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.3344 - accuracy: 0.9200 - val_loss: 0.4876 - val_accuracy: 0.8325\n",
      "Epoch 34/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3211 - accuracy: 0.9225 - val_loss: 0.4831 - val_accuracy: 0.8325\n",
      "Epoch 35/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3119 - accuracy: 0.9244 - val_loss: 0.4799 - val_accuracy: 0.8313\n",
      "Epoch 36/40\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3028 - accuracy: 0.9262 - val_loss: 0.4765 - val_accuracy: 0.8331\n",
      "Epoch 37/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2928 - accuracy: 0.9298 - val_loss: 0.4731 - val_accuracy: 0.8331\n",
      "Epoch 38/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2858 - accuracy: 0.9320 - val_loss: 0.4713 - val_accuracy: 0.8325\n",
      "Epoch 39/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2733 - accuracy: 0.9358 - val_loss: 0.4689 - val_accuracy: 0.8325\n",
      "Epoch 40/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2678 - accuracy: 0.9372 - val_loss: 0.4673 - val_accuracy: 0.8306\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, validation_data=val_ds,\n",
    "    epochs=epochs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T18:38:34.545400600Z",
     "start_time": "2023-09-21T18:38:01.497204500Z"
    }
   },
   "id": "e3df2010520df33c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e71a32c0f2e2fdb"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 5s 18ms/step - loss: 0.5245 - accuracy: 0.8029\n",
      "Loss =  0.5244626998901367\n",
      "Accuracy =  0.8028749823570251\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss = \", loss)\n",
    "print(\"Accuracy = \", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:47:59.706392100Z",
     "start_time": "2023-09-21T19:47:55.045655800Z"
    }
   },
   "id": "d9d05e99eb8af459"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T19:48:19.726121200Z",
     "start_time": "2023-09-21T19:48:19.647535800Z"
    }
   },
   "id": "3132379db072c60f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1bd3b775c11776"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
